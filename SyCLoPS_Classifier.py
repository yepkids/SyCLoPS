print("\nThis is the SyCLoPS classifier program. This program formats the input catalog, calculates LPSAREA, classifies each LPS node according to the conditions, and outputs the classified catalog.\n")
print("Please direct any questions to the author of this script: Yushan Han (yshhan@ucdavis.edu)\n")
print("Version: 2025-12-01 \n")
print("PyArrow (https://arrow.apache.org/docs/python/install.html) is needed to read/write parquet files in this program. Parquet files are faster to read/write and they take up less space on disk.\n")
print('The "multiprocess" package (https://pypi.org/project/multiprocess) is needed to enable efficient memory sharing of large data structure across processors to perform the parallel computation in this script.\n')
print('Please change the constants and files names accordingly in the first part of the script before running the prorgam.\n')
import pandas as pd 
import numpy as np
import xarray as xr
import math
import time
from scipy.spatial import cKDTree
from scipy import stats
import multiprocess as ma
import cftime
import warnings

#--------Constants and File Naming (Change Accordingly)-------#
nprocess=6 # Number of processors to use for parallel computation in this program
model_data_name = "ERA5" #A short name for the climate dataset you are using. This name will be used to form the names of input/output files in this program.
TETrackFile=f'out_track/{model_data_name}_tracks_range60.csv' #The StitchNodes output file in CSV format. Required before running the program.
InputFileName=f'classified_track/inputfile/SyCLoPS_input_{model_data_name}.parquet' #Choose the SyCLoPS input LPS catalog filename. This file will be generated by the program.
SizeBlobStatFile=f'blobstats/{model_data_name}_size_blob_stats.txt' #Enter a filename for the output of TE's BlobStats. 
QStrackFileName=f'other_info/{model_data_name}_QS_track_info.csv' #Choose a filename for the output quasi-stationary track information. This file will be generated by the program.
ClassifiedOutFile=f'classified_track/SyCLoPS_classified_{model_data_name}.parquet' #Choose the output filename of the final classified dataset.
ZSFile='ZSfile_general.nc' #The ERA5 invariant surface geopotential file (0.25 deg) which can be used for any datasets in the classifier. No need to change this.
grid_res=0.25*0.25 #LAT x LON, the CMIP model nominal resolution in deg^2, or the approximate area of Healpix grid cells in deg^2.
#------------------Functions--------------------#
# The track_spread function below calculates the track spread of each track.
# Track spread is the standard deviation of the distance between each node in a track and the first track node
def round_to_nearest5(x):
    return round(x / 5) * 5

def track_spread(k): 
    T=cKDTree(list(zip(X[FST[k]:LST[k]+1],Y[FST[k]:LST[k]+1],Z[FST[k]:LST[k]+1])))
    distspr=np.std(T.query((X[FST[k]],Y[FST[k]],Z[FST[k]]),k=LST[k]-FST[k]+1)[0])/(np.pi/180)
    return distspr

def track_percor(k): #Calculates the Pearson correlation coefficient of all the nodes within a track
    with warnings.catch_warnings():
        warnings.filterwarnings(action='ignore', message='An input array is constant; the correlation coefficient is not defined')
        percor=abs(stats.pearsonr(LON[FST[k]:LST[k]+1], LAT[FST[k]:LST[k]+1]).statistic)
        if math.isnan(percor):
            percor=0
        return percor

def zsmax(k): #Calculates the maximum surface geopotential within 1 degree of an LPS node
    idx = Tz.query_ball_point((X[k],Y[k],Z[k]),r=1*(np.pi/180))
    zmax=dszsnf[idx].max()
    return zmax

def zsper(k): #Calculates a lower-terrain ratio to adjust raw LPS size (area).
    idx = Tz.query_ball_point((X[k],Y[k],Z[k]),r=5.0*(np.pi/180))
    zper=len(np.where(dszsnf[idx]<7000)[0])/len(dszsnf[idx])
    return zper

#The blobpairing function below outputs a blob index and a node index in tuples
def blobpairing(k):
    nodepair=[]
    NodeTimeArr=np.array(NodeTimeidx.loc[BlobTime[k]])
    if NodeTimeArr.size>0:
        T=cKDTree(list(zip(X[NodeTimeArr],Y[NodeTimeArr],Z[NodeTimeArr])))
        dft=dfin0.iloc[NodeTimeArr] #At the same timestep for blobs and nodes
        for i2 in BlobTimeidx[k]: 
            #First, pair blobs with nodes that are within 5 degrees GCD of their centroids:
            idx=T.query_ball_point((Xb[i2],Yb[i2],Zb[i2]),r=5*(np.pi/180))
            if len(idx)>1:        
                node=dfin0.MSLP.iloc[NodeTimeArr[idx]].idxmin()
                nodepair.append((i2,node))
            elif len(idx)==1:
                node=NodeTimeArr[idx[0]]
                nodepair.append((i2,node))
            else:
                #If blobs are not paired with any nodes at this point, pair nodes that are bounded by the extent of the blobs:
                if dfblob.maxlon[i2]-dfblob.minlon[i2]>180:
                    nid=dft[((dft.LON>=350)&(dft.LON<360))|((dft.LON>=0)&(dft.LON<=10))\
                        &(dft.LAT>=dfblob.minlat[i2])&(dft.LAT<=dfblob.maxlat[i2])].index.values
                else:
                    nid=dft[(dft.LON>=dfblob.minlon[i2])&(dft.LON<=dfblob.maxlon[i2])\
                        &(dft.LAT>=dfblob.minlat[i2])&(dft.LAT<=dfblob.maxlat[i2])].index.values
                if len(nid)>0:
                    node=dfin0.MSLP.iloc[nid].idxmin()
                    nodepair.append((i2,node))
    else:
        pass
    return nodepair

#---------------Main Program Starts----------------#
if __name__ == '__main__':
    #-------------User inputs and tips---------------#
    long_str=(
        "Enter a mode number below:\n"
        "0: No skipping any codes (Default)\n"
        "1: Skip the optional QS Track operations and labeling QS tracks in the classification.\n"
        "2: Skip LPSAREA operations and simplify extratropical branch classification (SC, EX and DSE only). You may choose this option if you are not interested in classifying TLCs.\n"
        "3: Skip QS Track and LPSAREA operation and simplify extratropical branch classification (1 & 2 combined).\n"
    )
    print(long_str)
    while True:
        modenum = input("Enter a mode number (leaving blank is 0):")
        if modenum == '':
            modenum = 0
        else:
            modenum = int(modenum)
        if 0 <= modenum <= 3:
            print("You entered mode number:", modenum)
            break
        else:
            print("Invalid mode number. Please enter a number between 0 and 3.")
        
    long_str=(
        "Please check the units of your input geopotential/height file for DetectNodes. Choose a unit from below.\n"
        "1: m**2 s**-2 (geopotential)\n"
        "2: m (meter)\n"
    )
    print(long_str)
    while True:
        unitnum = input("Enter a number (Default is 1):")
        if unitnum == '':
            unitnum = 1
        else:
            unitnum = int(unitnum)
        if 1 <= unitnum <= 2:
            print("You entered number:", unitnum)
            break
        else:
            print("Invalid mode number. Please enter a number between 0 and 4.")
    zgconv=1
    if unitnum==2:
        zgconv=1/9.8
 
    print("\nAre you using 250 hPa data to replace 300 and 200 hPa data that will be used in the classification? (This is common for climate model data outputs)")
    while True:
        data250=input("Enter Y/y (Yes) or the DEFAULT N/n (No):")
        if data250 == '':
            data250='N'
        if data250 == 'Y' or data250 == 'N' or data250 == 'y' or data250 == 'n':
            print("You entered:",data250)
            if data250 == 'Y' or data250 == 'y':
                print("If you use 250 hPa data, it is recommended for you to change the name of parameter 'WS200PMX' in this script to 'WS250PMX' to avoid confusion")
            break
        else:
            print("Invalid Entry. Please read the insturction and enter again.")

    print("\nAre you using a regional dataset? If the answer is yes, the classification program will use alternatives to replace WS200MAX criteria (see SyCLoPS SI text S4 for details). ")
    while True:
        isregion=input("Enter Y/y (Yes) or the DEFAULT N/n (No):")
        if isregion == '':
            isregion='N'
        if isregion == 'Y' or isregion == 'N' or isregion == 'y' or isregion == 'n':
            print("You entered:",isregion)
            break        
        else:
            print("Invalid Entry. Please read the insturction and enter again.")
            
    print("\nWhat's the time resolution of your LPS tracks in hours? The allowed range is 1-6")
    while True:
        timeres = input ("Enter a value (leaving blank is the default '3'):")
        if timeres == '':
            timeres=3
        else:
            timeres=int(timeres)
        if 1<=timeres<=6:
            print("You entered:",timeres,'hours')
            break
        else:
            print("You entered a time resolution greater than 6 hrs or less than 1 hr. Please confirm that the track data meet the time resolution requirement and enter again.")  
    
    convrate=3/int(timeres)
    if grid_res<=0.25*0.25:
        grid_res=0.25*0.25
    elif grid_res>1.5*1.5:
        grid_res=1.5*1.5
        print("\nWarning: The grid resolution you entered is too coarse for tropical cyclone (TC) classification. The program will use 1.5 deg^2 as the lowest grid resolution for TC classification instead.\n")

    #---------------Data Preparation----------------#
    print("\nData preparation and preprocessing starts...") ;startt=time.time()
    # Conversion of the output TE csv file into the required format for classification
    dfin = pd.read_csv(TETrackFile,na_values=' nan')
    dfin.columns = dfin.columns.str.strip()
    #Combine four time columns to form the ISOTIME column
    try:
        dfin.insert(5, 'ISOTIME', pd.to_datetime(dict(year=dfin.year, month=dfin.month, day=dfin.day,hour=dfin.hour)))
    except: # In case of using cftime for 360_day calendar
        dfin['ISOTIME'] = [cftime.Datetime360Day(y, m, d, h) for y, m, d, h in zip(dfin['year'], dfin['month'], dfin['day'], dfin['hour'])]
        dfin['ISOTIME'] = dfin['ISOTIME'].astype(str)

    dfin=dfin.drop(columns=['year','month','day','hour'])
    dfin=dfin.rename(columns={"lon": "LON", "lat": "LAT","track_id":"TID"})
    if 2<=modenum<=3:
        dfin.to_parquet(InputFileName) #Save the final form of the input catalog
    #Open and read the constant surface geopotential variable of a climate dataset
    dszs=xr.open_dataset(ZSFile).ZS
    lonz=dszs.longitude.to_numpy()%360*(np.pi/180)
    latz=dszs.latitude.to_numpy()%360*(np.pi/180)
    dszsnf=dszs.to_numpy().flatten()
    
    #Identify the start and end index of each track
    FST = np.unique(dfin.TID.values, return_index=1)[1]
    LST=len(dfin)-np.unique(dfin.TID.values[::-1], return_index=1)[1]-1
    #Convert longitudes and latitudes in both the input LPS catalog and blob stats dataset to the spherical coordinates (X,Y,Z) for implementing KDTrees:
    LAT=np.array(dfin.LAT)%360;LON=np.array(dfin.LON)%360
    X=np.cos(LON*(np.pi/180))*np.cos(LAT*(np.pi/180))
    Y=np.sin(LON*(np.pi/180))*np.cos(LAT*(np.pi/180))
    Z=np.sin(LAT*(np.pi/180))
    coordinates = []
    for i in latz:
        for j in lonz:
            coordinates.append((np.cos(j) * np.cos(i),np.sin(j) * np.cos(i),np.sin(i)))
    pts = np.array(coordinates)
    Tz = cKDTree(pts) 
    endt=time.time()
    print("Time lapsed (s) for the data preparation section: "+ str(endt-startt)) 
    
    #--------------------LPSAREA--------------------#
    if modenum==0 or modenum==1:
        print("\nLPSAREA computation starts...") ;startt = time.time()
        #Perform the functions to pair size blobs to LPS nodes and compute LPSAREA
        #To save time, multiprocessing is recommended. Single-threaded may take ~2hrs for 9.5 million blobs/7.8 million nodes.
        #Multiprocessing may take up more physical memroy (~ 7GB with 64 threads and 9.5 million blobs/7.8 million nodes)
        #Open and format the size blob statistics file output by TE's BlobStats
        dfblob=pd.read_csv(SizeBlobStatFile,sep="\t", header=None)
        dfblob=dfblob.drop(dfblob.columns[[1]], axis=1)
        #dfblob=pd.read_csv(SizeBlobStatFile,index_col=0)
        try:
            dfblob.columns=["blobid","time","centlon","centlat","minlat","maxlat","minlon","maxlon","blobsize","ike"]
        except:
            dfblob.columns=["blobid","time","centlon","centlat","minlat","maxlat","minlon","maxlon","blobsize"]
        dfblob['time']=pd.to_datetime(dfblob['time'])
        #Generate a list of node and blob index of each unique timestamp for the later blob pairing process
        temp_index0=np.arange(0,len(dfin),1);dfin['ind']=temp_index0
        NodeTimeidx=dfin.groupby(dfin['ISOTIME'])['ind'].apply(list)
        temp_index1=np.arange(0,len(dfblob),1);dfblob['ind']=temp_index1
        BlobTime=pd.unique(dfblob['time'])
        BlobTimeidx=np.array(dfblob.groupby(dfblob['time'])['ind'].apply(list))
        LonB=np.array(dfblob.centlon)
        LatB=np.array(dfblob.centlat)
        Xb=np.cos(LonB*(np.pi/180))*np.cos(LatB*(np.pi/180))
        Yb=np.sin(LonB*(np.pi/180))*np.cos(LatB*(np.pi/180))
        Zb=np.sin(LatB*(np.pi/180))
        dfin0=dfin[['ISOTIME','LON','LAT','MSLP']]
        with ma.Pool(nprocess) as pool_obj:
            nodepair_list=pool_obj.map(blobpairing,range(len(BlobTime)))
            zsperl=pool_obj.map(zsper,range(len(dfin)))
        #processing the output tuple list and pair LSP nodes to blobs.
        nodepair_list=np.concatenate(nodepair_list)
        dfblob['paired_node']=-1
        blobidx=[i[0] for i in nodepair_list] 
        nodematched=[i[1] for i in nodepair_list]
        dfblob.loc[blobidx,'paired_node']=nodematched #Indices of paried nodes of each blob

        #Calculate the raw size of each LPS nodes by the sizes of paired blobs:
        sizecol=np.round(dfblob[dfblob['paired_node']!=-1].groupby(dfblob['paired_node'])['blobsize'].sum()*1e-6)
        dfin['RAWAREA']=0;dfin.loc[sizecol.index.values,'RAWAREA']=sizecol.values
        try:
            ikecol=np.round(dfblob[dfblob['paired_node']!=-1].groupby(dfblob['paired_node'])['ike'].sum()*1e-12)
            dfin['IKE']=0; dfin.loc[ikecol.index.values,'IKE']=ikecol.values
        except:
            pass
        #Adjust the raw LPS size to the final size (LPSAREA) according to the lower-terrain ratio:
        zsind=np.where((np.array(zsperl) >= 0.3)&(np.array(zsperl) <= 0.7))[0]
        adjsize=dfin.RAWAREA.copy()
        adjsize[zsind]=dfin.RAWAREA[zsind]*2
        dfin['LPSAREA']=adjsize
        dfin.to_parquet(InputFileName) #Save the final form of the input catalog
        endt = time.time()
        print("Time lapsed (s) for the LOWAREA section: "+ str(endt-startt))

    #----------------------QS-----------------------#
    #This part is reserved for computing information required for quasi-stationary (QS) track classification.
    #Computation in this part requires longer processing time if single-threaded (~1 hr for 380 thousands tracks).
    if modenum==0 or modenum==2:
        print("\nQS parameters calculation starts ...") ;startt = time.time()
        with ma.Pool(nprocess) as pool_obj: #Calcualte the track parameters for labeling QS tracks (parallel computing)
            distspr_list=pool_obj.map(track_spread,range(len(FST)))
            percor_list=pool_obj.map(track_percor,range(len(FST)))
            zmax_list=pool_obj.map(zsmax,range(len(dfin)))
        #To form a dataframe of track information for later use in labeling QS tracks.
        dfin['ZSMX']=zmax_list
        zsmx_ratio=dfin.groupby('TID')['ZSMX'].apply(lambda x: len(np.where(x>150)[0])/len(x))
        infodic={'TID':pd.unique(dfin.TID),'Track Linearity':percor_list,'Track Spread':distspr_list,'Track Inland Ratio':zsmx_ratio}
        dfinfo=pd.DataFrame(infodic)
        #Save QS track information to a csv file for potential future usages:
        dfinfo.to_csv(QStrackFileName)
        endt=time.time()
        print("Time lapsed (s) for the QS section: "+ str(endt-startt))

    #--------------Main Classification--------------#
    ## Main Classification program starts (the whole process takes ~20 secs to complete for ~8 million nodes)
    print("\nSyCLoPS main classification program starts ...") ;startt=time.time()
    dfin['mslcc_ratio']=dfin.MSLPCC20/dfin.MSLPCC55
    ## Generate arrays of placeholder labels
    Full_Name=np.array(["Non-labeled"]*len(dfin),dtype=object)
    short_label=np.array(["NLB"]*len(dfin),dtype=object)
    rhconv=1
    if dfin.RH850AVG[dfin.RH850AVG<1e14].max()<2:
        rhconv=0.01

    ## High-altitude Branch Labeling
    try:
        cond_hal=(dfin.Z850<dfin.ZS) | (dfin.Z850-dfin.ZS<100) #High-altitude Condition. Z850 is not necessary for classification if data contains missing values (typically 1e20 or 1e15) or NaN. T850 will be used.
    except:
        cond_hal=(dfin.T850!=dfin.T850) | (abs(dfin.T850)>1e14) | (dfin.T850==0)
    df_hatl=dfin[(cond_hal) & ((dfin.MIDTKCC<0)|(dfin.UPPTKCC<0))]; hatl_id=df_hatl.index.values #nodes that satisfy criteria of HATL. Check for missing values is added.
    df_hal=dfin[(cond_hal) & ~((dfin.MIDTKCC<0)|(dfin.UPPTKCC<0))]; hal_id=df_hal.index.values #nodes that satisfy criteria of HAL. Check for missing values is added.
    Full_Name[hatl_id]="High-altitude Thermal Low"; short_label[hatl_id]="HATHL"
    Full_Name[hal_id]="High-altitude Low"; short_label[hal_id]="HAL"
    
    ## Dryness Branch Labeling
    cond_dry=dfin.RH850AVG<=60*rhconv #Dryness Condition
    cond_cv=(((dfin.VO500AVG>0) & (dfin.LAT>=0.0)) | ((dfin.VO500AVG<0) & (dfin.LAT<0.0))) #Cyclonic Condition
    df_dsd=dfin[~(cond_hal) & (cond_dry) & ~(dfin.LOWTKCC<0)]; dsd_id=df_dsd.index.values #nodes that satisfy criteria of DSD
    df_dothl=dfin[~(cond_hal) & (cond_dry) & (dfin.LOWTKCC<0) & (cond_cv)]; dothl_id=df_dothl.index.values #nodes that satisfy criteria of DOTHL
    df_thl=dfin[~(cond_hal) & (cond_dry) & (dfin.LOWTKCC<0) & ~(cond_cv)]; thl_id=df_thl.index.values #nodes that satisfy criteria of THL
    Full_Name[dsd_id]="Dry Disturbance"; short_label[dsd_id]="DSD"
    Full_Name[dothl_id]="Deep (Orographic) Thermal Low"; short_label[dothl_id]="DOTHL"
    Full_Name[thl_id]="Thermal Low"; short_label[thl_id]="THL"
    
    ## Tropical Branch Labeling
    # TC Labeling
    if data250=='Y' or data250=='y':
        cond_trop=(dfin.RH100MAX>20*rhconv) & (dfin.DEEPSHEAR<13) & (dfin.T850>280) 
        cond_tc=(dfin.UPPTKCC<-117.6*zgconv) & (dfin.LOWTKCC<0) & (dfin.MSLPCC20>round_to_nearest5(-107*grid_res+247)) 
    else:
        cond_trop=(dfin.RH100MAX>20*rhconv) & (dfin.DEEPSHEAR<18) & (dfin.T850>280) #Default Tropical Condition
        cond_tc=(dfin.UPPTKCC<-107.8*zgconv) & (dfin.LOWTKCC<0) & (dfin.MSLPCC20>round_to_nearest5(-85*grid_res+220)) #Default Tropical Cyclone Condition
        
    cond_td=(dfin.MSLPCC55>160) & (dfin.UPPTKCC<0)  #Tropical Depression Condition
    cond_md=(dfin.RH850AVG>85*rhconv) & (dfin.U850DIFF>0)  #Monsoon System Condition
    df_dst=dfin[~(cond_hal) & ~(cond_dry) & (cond_trop) & ~(cond_cv)]; dst_id=df_dst.index.values #nodes that satisfy criteria of DST
    df_tc=dfin[~(cond_hal) & ~(cond_dry) & (cond_cv)  &(cond_trop) & (cond_tc)]; tc_id=df_tc.index.values #nodes that satisfy criteria of TC
    df_tdmd=dfin[~(cond_hal) & ~(cond_dry) & (cond_cv) & (cond_trop)& ~(cond_tc) & (cond_td) & (cond_md)]
    tdmd_id=df_tdmd.index.values #nodes that satisfy criteria of TD(MD)
    df_tdew=dfin[~(cond_hal) & ~(cond_dry) & (cond_cv) & (cond_trop)& ~(cond_tc) & (cond_td) & ~(cond_md)]
    tdew_id=df_tdew.index.values #nodes that satisfy criteria of TD
    df_tloml=dfin[~(cond_hal) & ~(cond_dry) & (cond_cv) & (cond_trop) & ~(cond_tc) & ~(cond_td) & (cond_md)]
    tloml_id=df_tloml.index.values #nodes that satisfy criteria of TLO(ML)
    df_tloew=dfin[~(cond_hal) & ~(cond_dry) & (cond_cv) & (cond_trop) & ~(cond_tc) & ~(cond_td) & ~(cond_md)]
    tloew_id=df_tloew.index.values #nodes that satisfy criteria of TLO
    df_ms=dfin[~(cond_hal) & ~(cond_dry) & (cond_cv) & (cond_trop) & ~(cond_tc) & (cond_md)]
    Full_Name[dst_id]="Tropical Disturbance"; short_label[dst_id]="DST"
    Full_Name[tc_id]="Tropical Cyclone"; short_label[tc_id]="TC"
    Full_Name[tdmd_id]="Tropical Depression(Monsoon Depression)"; short_label[tdmd_id]="TD(MD)"
    Full_Name[tdew_id]="Tropical Depression"; short_label[tdew_id]="TD"
    Full_Name[tloml_id]="Tropical Low (Monsoon Low)"; short_label[tloml_id]="TLO(ML)"
    Full_Name[tloew_id]="Tropical Low"; short_label[tloew_id]="TLO"
    
    ## Extratropical Branch Labeling
    df_dse=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & ~(cond_cv)]; dse_id=df_dse.index.values #nodes that satisfy criteria of DSE
    Full_Name[dse_id]="Extratropical Disturbance"; short_label[dse_id]="DSE"
    
    WS200PMXadj=0
    if data250=='Y' or data250=='y':
        WS200PMXadj=5
    if isregion=='N' or isregion=='n':  
        cond_sc=(dfin.LOWTKCC<0)&(dfin.Z500CC>0)&(dfin.WS200PMX>30+WS200PMXadj) #SC Condition
    else:
        # The alternative criteria to replace WS200MAX criteria for SC condition in regional models (See SI text S4):
        cond_sc=(dfin.LOWTKCC<0)&(dfin.Z500CC>0)&((dfin.WS200PMX>30+WS200PMXadj)|(dfin.DEEPSHEAR>12)) 
        #OR (dfin.LOWTKCC<0)&(dfin.Z500CC>0)&((dfin.WS200PMX>30+WS200PMXadj)|((dfin.T850>273)&(dfin.DEEPSHEAR>14))) 
    if modenum==2 or modenum==3:
        df_sc=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & (cond_sc)]; sc_id=df_sc.index.values #nodes that satisfy criteria of SC
        df_ex=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & ~(cond_sc)]; ex_id=df_ex.index.values #nodes that satisfy criteria of EX
        Full_Name[sc_id]="Subtropical Cyclone"; short_label[sc_id]="SC"
        Full_Name[ex_id]="Extratropical Cyclone"; short_label[ex_id]="EX"
    else:
        cond_tlc= (dfin.MIDTKCC<0) & (dfin.LOWTKCC<0) & (((dfin.LPSAREA<=5.5e5) & (dfin.MSLPCC20>190) & (dfin.LPSAREA>0)) | ((dfin.MSLPCC20>420) & (dfin.mslcc_ratio>0.5))) #TLC Condition
        #The alternative TLC condition without using the embedded TLC criterion (See Sec. 5.3)
        #cond_tlc= (dfin.MIDTKCC<0) & (dfin.LOWTKCC<0) & (dfin.LPSAREA<=7e5) & (dfin.MSLPCC20>145) & (dfin.LPSAREA>0)
        df_tlc=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & (cond_tlc)] #nodes that satisfy TLC Condition
        if (isregion=='Y' or isregion=='y'): 
        # The alternative criteria to replace WS200MAX criteria for TLC condition in regional models (See SI text S4):
            df_stlc=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & ((dfin.WS200PMX>=25+WS200PMXadj)|(dfin.DEEPSHEAR>11)) & (cond_tlc)] 
            # OR df_stlc=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & ((dfin.WS200PMX>=25+WS200PMXadj)|((dfin.DEEPSHEAR>11)&(dfin.T850>273))) & (cond_tlc)] 
            df_pl=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv)  & ((dfin.WS200PMX<25+WS200PMXadj)|(dfin.DEEPSHEAR<=11)) & (cond_tlc)] 
            # OR df_stlc=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & ((dfin.WS200PMX<25+WS200PMXadj)|((dfin.DEEPSHEAR<=11)&(dfin.T850<=273))) & (cond_tlc)] 
        else: #The default condition
            df_stlc=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & (dfin.WS200PMX>=25+WS200PMXadj) & (cond_tlc)]
            df_pl=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv)  & (dfin.WS200PMX<25+WS200PMXadj) & (cond_tlc)]
        stlc_id=df_stlc.index.values #nodes that satisfy criteria of STLC
        pl_id=df_pl.index.values #nodes that satisfy criteria of PL
        df_sc=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & ~(cond_tlc) & (cond_sc)]
        sc_id=df_sc.index.values #nodes that satisfy criteria of SC
        df_ex=dfin[~(cond_hal) & ~(cond_dry) & ~(cond_trop) & (cond_cv) & ~(cond_tlc) & ~(cond_sc)]
        ex_id=df_ex.index.values #nodes that satisfy criteria of EX
        Full_Name[stlc_id]="Subtropical Tropical-like Cyclone (Subtropical Storm)"; short_label[stlc_id]="SS(STLC)"
        Full_Name[pl_id]="Polar Low (Polar Tropical-like Cyclone)"; short_label[pl_id]="PL(PTLC)"
        Full_Name[sc_id]="Subtropical Cyclone"; short_label[sc_id]="SC"
        Full_Name[ex_id]="Extratropical Cyclone"; short_label[ex_id]="EX"
    #Assign all the labels to the columns:
    dfin['Short_Label']=short_label
    dfin['Full_Name']=Full_Name

    ## Step TWO: Other information about the nodes and tracks
    #Denoting Binary tags for Tropical_Flag and Transition_Zone:
    cond_trans=(cond_trop) & ((dfin.RH100MAX<55*rhconv) | (dfin.DEEPSHEAR>10)) & (abs(dfin.LAT)>15)
    trans_flag=np.zeros(len(dfin))
    trop_flag=np.zeros(len(dfin))
    dfin['Transition_Zone']=trans_flag
    dfin['Tropical_Flag']=trop_flag
    dfin.loc[cond_trans,'Transition_Zone']=1
    dfin.loc[cond_trop,'Tropical_Flag']=1
    
    ## Track labeling
    if data250=='Y' or data250=='y':
        tctrack=pd.unique(df_tc.TID)[df_tc.groupby('TID')['LON'].count()>=round(6*convrate)]
    else:
        tctrack=pd.unique(df_tc.TID)[df_tc.groupby('TID')['LON'].count()>=round(8*convrate)]
    mstrack=pd.unique(df_ms.TID)[df_ms.groupby('TID')['LON'].count()>=round(10*convrate)]
    tctrack_id=dfin[dfin.TID.isin(tctrack)].index.values #TC Track   
    mstrack_id=dfin[dfin.TID.isin(mstrack)].index.values #MS Track
    # TLC tracks
    if modenum<=1: 
        tlctrack=pd.unique(df_tlc.TID)[df_tlc.groupby('TID')['LON'].count()>=round(2*convrate)]
        #STLC Track
        stlctrack=np.intersect1d(tlctrack,pd.unique(df_stlc.TID))
        stlctrack_id=dfin[dfin.TID.isin(stlctrack)].index.values
        #PL Track
        pltrack=np.intersect1d(tlctrack,pd.unique(df_pl.TID))
        pltrack_id=dfin[dfin.TID.isin(pltrack)].index.values
    # QS Track
    if modenum==0 or modenum==2:
        qstrack=pd.unique(dfinfo[(dfinfo["Track Linearity"]<0.55)&(dfinfo["Track Spread"]<3)&(dfinfo["Track Inland Ratio"]>0.65)].TID)
        qstrack_id=dfin[dfin.TID.isin(qstrack)].index.values
    
    ## Identifying extratropical transition (EXT) and tropical transition (TT) Nodes
    #EXT nodes
    dftc=dfin[dfin.TID.isin(tctrack)]
    extflag=np.zeros(len(tctrack)).astype(int)-1
    for c, i in enumerate(tctrack):
        df0=dftc[dftc.TID==i]
        lst_tc=df0[(df0.Short_Label=='TC')].index[-1]
        dfe=df0.loc[lst_tc+1:lst_tc+3]
        dfe=dfe[(dfe.Tropical_Flag==0) & (dfe.Short_Label != 'DSE')]
        if len(dfe)>0:
            fst_ex=dfe.index[0]
            extflag[c]=fst_ex
    extflag=np.delete(extflag, np.argwhere(extflag==-1))
    #TT nodes
    ttflag=np.zeros(len(tctrack)).astype(int)-1
    for c, i in enumerate(tctrack):
        df0=dftc[dftc.TID==i]
        fst_tc=df0[(df0.Short_Label=='TC')].index[0]
        dftc_pre=df0.loc[:fst_tc-1]
        if np.any(dftc_pre.Tropical_Flag==0):
            ttflag[c]=fst_tc
    ttflag= np.delete(ttflag, np.argwhere(ttflag==-1))
    
    ## Writing information for Track_Info
    track_label=np.array(["Track"]*len(dfin),dtype=object)
    track_label[tctrack_id]=track_label[tctrack_id]+'_TC'
    track_label[mstrack_id]=track_label[mstrack_id]+'_MS'
    if modenum<=1: 
        track_label[stlctrack_id]=track_label[stlctrack_id]+'_SS(STLC)'
        track_label[pltrack_id]=track_label[pltrack_id]+'_PL(PTLC)'
    track_label[extflag]=track_label[extflag]+'_EXT'
    track_label[ttflag]=track_label[ttflag]+'_TT'
    if modenum==0 or modenum==2:
        track_label[qstrack_id]=track_label[qstrack_id]+'_QS'
    dfin['Track_Info']=track_label
    # Adjusting (smoothing) Labels for TDs in stable TC periods.
    labels0=dfin.Short_Label.values
    labels=dfin.Short_Label.values.copy()
    fi=dftc.groupby('TID').head(1).index
    li=dftc.groupby('TID').tail(1).index
    for j in range(1,round(8*convrate+1)):
        for k in range(len(fi)):
            for i in range(fi[k]+j, li[k]-j+1):
                if (
                    all('TD' in label for label in labels0[i:i+j]) and 
                    all('TC' in label for label in labels0[i-j:i]) and 
                    all('TC' in label for label in labels0[i+j:i+2*j])
                ):
                    mslp_values= dftc.MSLP.loc[i-1:i+j].values
                    mslp_start = mslp_values[0]
                    mslp_end = mslp_values[-1]
                    mslp_int = np.linspace(mslp_start, mslp_end, j+2)
                    if abs(mslp_values-mslp_int).max() <= 500:
                        labels[i:i+j] = 'TC'
    dfin['Adjusted_Label'] = labels
    dfc_tc = dfin[(dfin['Track_Info'].str.contains('TC')) & (dfin.Adjusted_Label=='TC')].reset_index(drop=True)
    dfms = dfin[dfin.TID.isin(mstrack)] #Select MS tracks
    labels = dfin.Adjusted_Label.values.copy()
    fi=dfms.groupby('TID').head(1).index
    li=dfms.groupby('TID').tail(1).index
    for j in range(1,round(8*convrate+1)):
        for k in range(len(fi)):
            for i in range(fi[k]+j, li[k]-j+1):
                if (
                    all((label == 'TD' or label == 'TLO') for label in labels[i:i+j]) and
                    all('M' in label for label in labels[i-j:i]) and
                    all('M' in label for label in labels[i+j:i+2*j])
                ):
                    mslp_values= dfms.MSLP.loc[i-1:i+j].values
                    mslp_start = mslp_values[0]
                    mslp_end = mslp_values[-1]
                    mslp_int = np.linspace(mslp_start, mslp_end, j+2)
                    if abs(mslp_values-mslp_int).max() <= 500:
                        for idx in range(i, i+j):
                            if labels[idx] == 'TD':
                                labels[idx] = 'TD(MD)'
                            elif labels[idx] == 'TLO':
                                labels[idx] = 'TLO(ML)'                     
    dfin['Adjusted_Label'] = labels

    ## Output the LPS classified catalog
    desired_columns = ['TID', 'LON', 'LAT', 'ISOTIME', 'MSLP', 'WS', 'WS925', 'ZS', 'Short_Label', 'Adjusted_Label', 'Tropical_Flag', 'Transition_Zone', 'Track_Info', 'LPSAREA', 'IKE', 'i', 'j']
    available_columns = [col for col in desired_columns if col in dfin.columns]
    dfout = dfin[available_columns]
    dfout.to_parquet(ClassifiedOutFile)
    ## Optionally, you can save it as a csv file:
    #dfout.to_csv(ClassifiedOutFile_CSV)
    endt=time.time();print("Time lapsed (s) for the main classification section: "+ str(endt-startt))
    
    # Example data
    print ("\nReference table for LPS labels in the classified catalog:\n")
    data = [
        ("HATHL", "High-altitude Thermal Low"),
        ("HAL", "High-altitude Low"),
        ("DOTHL", "Deep (Orographic) Thermal Low"),
        ("THL", "Thermal Low"),
        ("DSD", "Dry Disturbance"),
        ("DOT", "Tropical Disturbance"),
        ("TC", "Tropical Cyclone"),
        ("TD(MD)", "Tropical Depression(Monsoon Depression)"),
        ("TD", "Tropical Depression"),
        ("TLO(ML)", "Tropical Low (Monsoon Low)"),
        ("TLO", "Tropical Low"),
        ("SS(STLC)", "Subtropical Tropical-like Cyclone (Subtropical Storm)"),
        ("PL(PTLC)", "Polar Low (Polar Tropical-like Cyclone)"),
        ("SC", "Subtropical Cyclone"),
        ("EX", "Extratropical Cyclone"),
        ("DSE", "Extratropical Disturbance"),
        ("MS", "Monsoonal System (tracks)"),
    ]

    # Print header
    print(f"{'Short Label':<15} {'Full Name':<30}")
    print("-" * 45)

    # Print rows
    for row in data:
        print(f"{row[0]:<15} {row[1]:<30}")
